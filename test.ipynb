{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(np.random.randn(10,3))\n",
    "df = df.applymap(lambda x: round(x*10,0))\n",
    "\n",
    "#np.random.randn(10,3)\n",
    "\n",
    "df.iloc[3:5,0] = np.nan\n",
    "df.iloc[4:6,1] = np.nan\n",
    "df.iloc[5:8,2] = np.nan\n",
    "df.info()\n",
    "sys.getsizeof(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numpy.ndarray"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    " Suggest params to maximize an objective function based on the\n",
    "    function evaluation history using a tree of Parzen estimators (TPE),\n",
    "    as implemented in the hyperopt package.\n",
    " \n",
    "    Use of this function requires that hyperopt be installed.\n",
    "    \"\"\"\n",
    "    # This function is very odd, because as far as I can tell there's\n",
    "    # no real documented API for any of the internals of hyperopt. Its\n",
    "    # execution model is that hyperopt calls your objective function\n",
    "    # (instead of merely providing you with suggested points, and then\n",
    "    # you calling the function yourself), and its very tricky (for me)\n",
    "    # to use the internal hyperopt data structures to get these predictions\n",
    "    # out directly.\n",
    " \n",
    "    # so they path we take in this function is to construct a synthetic\n",
    "    # hyperopt.Trials database which from the `history`, and then call\n",
    "    # hyoperopt.fmin with a dummy objective function that logs the value\n",
    "    # used, and then return that value to our client.\n",
    " \n",
    "    # The form of the hyperopt.Trials database isn't really documented in\n",
    "    # the code -- most of this comes from reverse engineering it, by\n",
    "    # running fmin() on a simple function and then inspecting the form of\n",
    "    # the resulting trials object.\n",
    "    if 'hyperopt' not in sys.modules:\n",
    "        raise ImportError('No module named hyperopt')\n",
    " \n",
    "    random = check_random_state(self.seed)\n",
    "    hp_searchspace = searchspace.to_hyperopt()\n",
    " \n",
    "    trials = Trials()\n",
    "    for i, (params, scores, status) in enumerate(history):\n",
    "        if status == 'SUCCEEDED':\n",
    "            # we're doing maximization, hyperopt.fmin() does minimization,\n",
    "            # so we need to swap the sign\n",
    "            result = {'loss': -np.mean(scores), 'status': STATUS_OK}\n",
    "        elif status == 'PENDING':\n",
    "            result = {'status': STATUS_RUNNING}\n",
    "        elif status == 'FAILED':\n",
    "            result = {'status': STATUS_FAIL}\n",
    "        else:\n",
    "            raise RuntimeError('unrecognized status: %s' % status)\n",
    " \n",
    "        # the vals key in the trials dict is basically just the params\n",
    "        # dict, but enum variables (hyperopt hp.choice() nodes) are\n",
    "        # different, because the index of the parameter is specified\n",
    "        # in vals, not the parameter itself.\n",
    " \n",
    "        vals = {}\n",
    "        for var in searchspace:\n",
    "            if isinstance(var, EnumVariable):\n",
    "                # get the index in the choices of the parameter, and use\n",
    "                # that.\n",
    "                matches = [i for i, c in enumerate(var.choices)\n",
    "                           if c == params[var.name]]\n",
    "                assert len(matches) == 1\n",
    "                vals[var.name] = matches\n",
    "            else:\n",
    "                # the other big difference is that all of the param values\n",
    "                # are wrapped in length-1 lists.\n",
    "                vals[var.name] = [params[var.name]]\n",
    " \n",
    "        trials.insert_trial_doc({\n",
    "            'misc': {\n",
    "                'cmd': ('domain_attachment', 'FMinIter_Domain'),\n",
    "                'idxs': dict((k, [i]) for k in hp_searchspace.keys()),\n",
    "                'tid': i,\n",
    "                'vals': vals,\n",
    "                'workdir': None},\n",
    "            'result': result,\n",
    "            'tid': i,\n",
    "            # bunch of fixed fields that hyperopt seems to require\n",
    "            'owner': None, 'spec': None, 'state': 2, 'book_time': None,\n",
    "            'exp_key': None, 'refresh_time': None, 'version': 0\n",
    "            })\n",
    " \n",
    "    trials.refresh()\n",
    "    chosen_params_container = []\n",
    " \n",
    "    def mock_fn(x):\n",
    "        # http://stackoverflow.com/a/3190783/1079728\n",
    "        # to get around no nonlocal keywork in python2\n",
    "        chosen_params_container.append(x)\n",
    "        return 0\n",
    " \n",
    "    fmin(fn=mock_fn, algo=tpe.suggest, space=hp_searchspace, trials=trials,\n",
    "         max_evals=len(trials.trials)+1,\n",
    "         **self._hyperopt_fmin_random_kwarg(random))\n",
    "    chosen_params = chosen_params_container[0]\n",
    " \n",
    "    return chosen_params\n",
    "More Examples\n",
    "hyperopt\n",
    "sys\n",
    "django\n",
    "Requests\n",
    "Scrapy\n",
    "SQLAlchemy\n",
    "Twisted\n",
    "NumPy\n",
    "mock\n",
    " \n",
    "Copyright Â© 2018 Program Talk.Omega- WordPress Theme by T Tak\n",
    "Cookies help us deliver our services. By using our services, you agree to our use "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.  1.  2.  3.  4.  5.  6.  7.  8.  9. 10. 11. 12. 13. 14. 15. 16. 17.\n",
      "  18. 19. 20. 21. 22. 23. 24. 25. 26. 27. 28. 29. 30. 31. 32. 33. 34. 35.\n",
      "  36. 37. 38. 39. 40. 41. 42. 43. 44. 45. 46. 47. 48. 49. 50. 51. 52. 53.\n",
      "  54. 55. 56. 57. 58. 59. 60. 61. 62. 63. 64. 65. 66. 67. 68. 69. 70. 71.\n",
      "  72. 73. 74. 75. 76. 77. 78. 79. 80. 81. 82. 83. 84. 85. 86. 87. 88. 89.\n",
      "  90. 91. 92. 93. 94. 95. 96. 97. 98. 99.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "   0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "   0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "   0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "   0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "   1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([80., 62., 39.,  7.,  3., 91., 97., 98., 95., 99.])"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = np.hstack((np.zeros(90), np.ones(10)))\n",
    "y2 = np.vstack((np.arange(len(y)),y))\n",
    "print(y2)\n",
    "zeros = y2[0][y2[1]==0]\n",
    "ones = y2[0][y2[1]==1]\n",
    "zero_index = np.random.choice(len(zeros), 5, replace=False)\n",
    "one_index = np.random.choice(len(ones), 5, replace=False)\n",
    "index = np.hstack((zeros[zero_index], ones[one_index]))\n",
    "index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7686654362734776\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'shape'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-55-caf51fcd9ffc>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfilter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m<\u001b[0m\u001b[0mm\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0ma\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mb\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m \u001b[0ma\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m: 'list' object has no attribute 'shape'"
     ]
    }
   ],
   "source": [
    "a = [0.7686654362734776, 0.7801642451042325, 0.775056874718723, 0.7703169707554532, 0.7693549519197025, 0.7686654362734776, 0.7686654362734776, 0.7686654362734776, 0.7686654362734776, 0.7686654362734776, 0.7686654362734776, 0.7686654362734776, 0.7686654362734776, 0.7686654362734776, 0.7686654362734776]\n",
    "m = np.median(a)\n",
    "print(m)\n",
    "b = filter(lambda x: x<m, a)\n",
    "list(b)\n",
    "a.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.2800000000000002"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def calc_survive_fraction(instances_count, steps_left):\n",
    "    return (1 / instances_count) ** (1 / steps_left)\n",
    "        \n",
    "#(1/400)**(1/9)\n",
    "#400*0.51**9\n",
    "calc_survive_fraction(90,((300-104)*0.8-20)/60)\n",
    "((300-104)*0.8-20)/60"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = np.array([[1,2],[3,4]])\n",
    "len(a[:,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "\n",
    "file_name = r'..\\check_8_c\\train.csv'\n",
    "df = pd.read_csv(file_name, nrows=100, low_memory=False )\n",
    "df_X = df.drop('target', axis=1)\n",
    "number_columns = [\n",
    "    col_name\n",
    "    for col_name in df_X.columns\n",
    "    if col_name.startswith('number')\n",
    "]\n",
    "df_X = df_X[number_columns]\n",
    "df_X.fillna(-1, inplace=True)\n",
    "\n",
    "pca = PCA()\n",
    "pca\n",
    "#pca.fit_transform(df_X)\n",
    "#expl_var[ expl_var > max()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "expl_var_ratio = pca.explained_variance_ratio_\n",
    "i = 0\n",
    "sum_ratio = 0\n",
    "for x in expl_var_ratio:\n",
    "    sum_ratio +=x\n",
    "    i += 1\n",
    "    if sum_ratio >.99:\n",
    "        break\n",
    "print(i)\n",
    "np.sum(expl_var_ratio[:i])\n",
    "len(pca.components_[0])\n",
    "#expl_var = pca.explained_variance_\n",
    "#max_expl_var = max(expl_var)\n",
    "#expl_var[ expl_var > max_expl_var * 0.01]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from xgboost import XGBRegressor\n",
    "model = XGBRegressor()\n",
    "a = {'a':1, 'b':2}\n",
    "\n",
    "import os\n",
    "d = pd.DataFrame([a])\n",
    "header = not os.path.isfile('metrics.csv')\n",
    "d.to_csv('metrics.csv', mode='a', index=False,header=header )\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "r = LogisticRegression()\n",
    "r.predict()\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "file_name = r'C:\\ds\\sdsj\\check_8_c\\train.csv'\n",
    "file_size = os.path.getsize(file_name)\n",
    "\n",
    "\"\"\"\n",
    "df = pd.read_csv(file_name, nrows=100 )\n",
    "df.to_csv('test_row_size.csv', header=False)\n",
    "row_size = os.path.getsize('test_row_size.csv')\n",
    "print('rows estimation', file_size/row_size)\n",
    "\n",
    "usecols=['number_0','wefwefwewe']\n",
    "usecols = df.columns & usecols\n",
    "print(usecols)\n",
    "\"\"\"\n",
    "\n",
    "#df = pd.read_csv(file_name, usecols=usecols, nrows=1000 )\n",
    "df = pd.read_csv(file_name, nrows=1000 )\n",
    "#corr = df[ :df.shape[1] ].corr()\n",
    "#df_corr = pd.DataFrame( corr )\n",
    "\n",
    "#corr_cols = set()\n",
    "#for i in range(df_corr.shape[0]):\n",
    "#    row = df_corr.iloc[i]    \n",
    "#    print(row[row>0.95])\n",
    "\n",
    "    #cols = row.filter(lambda x: abs(x) > 0.95)\n",
    "#        if abs(v) > hyper_params_corr_limit and i != j:\n",
    "#            corr_cols[corr.columns[j]] = True            \n",
    "\n",
    "print(corr_cols)\n",
    "\n",
    "#df = pd.read_csv(r'C:\\ds\\sdsj\\check_8_c\\train.csv', nrows=10 )\n",
    "#df.shape\n",
    "#round(1234,-2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df.apply(lambda x: x.fillna(x.mean()), axis=0)\n",
    "#df.fillna\n",
    "\n",
    "df_X = df.copy()\n",
    "df_X[2].value_counts().index[0]\n",
    "\n",
    "#df_X[1].fillna( df_X[1].max(), inplace=True)\n",
    "#df_X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train\n",
    "argv = ['--train-csv', r'..\\check_1_r\\train.csv',\n",
    "       '--model-dir', r'.',\n",
    "       '--mode', 'regression']\n",
    "print(' '.join(argv) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#predict\n",
    "argv = ['--test-csv', r'..\\check_1_r\\test.csv',\n",
    "        '--prediction-csv', r'..\\check_1_r\\prediction.csv', \n",
    "       '--model-dir', r'.']\n",
    "print(' '.join(argv) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#score\n",
    "argv = ['--test-target-csv', r'..\\check_1_r\\test-target.csv',\n",
    "       '--prediction-csv', r'..\\check_1_r\\h2o_prediction.csv']\n",
    "print(' '.join(argv) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "import pandas as pd\n",
    "import sys\n",
    "import os\n",
    "#from tpot import TPOTClassifier, TPOTRegressor\n",
    "#tpotr = TPOTRegressor()\n",
    "df = pd.read_csv(r'..\\check_8_c\\train.csv', nrows=None, low_memory=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "#if any(df.isnull()):    \n",
    "#   df.fillna(-1, inplace=True)\n",
    "\n",
    "print(df.isnull().values.any())\n",
    "df.fillna(-1, inplace=True)\n",
    "print(df.isnull().values.any())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info(memory_usage='deep')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "def transform_ds(df):\n",
    "    int_cols = []\n",
    "    float_cols = []\n",
    "    category_cols = []\n",
    "    other_cols = []\n",
    "    \n",
    "    df_onehot = pd.DataFrame()\n",
    "\n",
    "    for col_name in df.columns:\n",
    "        \n",
    "        n_uniq = df[col_name].nunique()\n",
    "        if n_uniq == 1: # skip constant columns            \n",
    "            continue\n",
    "            \n",
    "        col_type = df.dtypes[col_name]\n",
    "        if col_type in ['int','int64']:\n",
    "            int_cols.append(col_name)\n",
    "        elif col_type in ['float', 'float64']:\n",
    "            float_cols.append(col_name)\n",
    "        elif col_type == 'object':\n",
    "            uniq_values = df[col_name].unique()\n",
    "            total = len( df[col_name] )\n",
    "            if 2 < n_uniq <= 20:\n",
    "                for uniq in uniq_values:\n",
    "                    df_onehot['onehot_{}={}'.format(col_name,uniq)] = (df[col_name] == uniq).astype(int)\n",
    "            elif n_uniq / total < 0.5: \n",
    "                category_cols.append(col_name)\n",
    "            else:\n",
    "                other_cols.append(col_name)\n",
    "        else:\n",
    "            other_cols.append(col_name)\n",
    "\n",
    "    df_opt = df_onehot.apply(pd.to_numeric, downcast='integer')\n",
    "    \n",
    "    if len(int_cols) > 0:\n",
    "        df_opt[int_cols] =  df[int_cols].apply(pd.to_numeric, downcast='integer')\n",
    "    \n",
    "    if len(float_cols) > 0:\n",
    "        df_opt[float_cols] = df[float_cols].apply(pd.to_numeric, downcast='float')\n",
    "        \n",
    "    if len(category_cols) > 0:\n",
    "        df_opt[category_cols] = df[category_cols].astype('category')    \n",
    "        \n",
    "    if len(other_cols) > 0:\n",
    "        df_opt[other_cols] = df[other_cols]\n",
    "\n",
    "    return df_opt\n",
    "\n",
    "df_opt = transform_ds(df)\n",
    "df_opt.info(memory_usage='deep')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_opt.dtypes.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "from sklearn.feature_selection import VarianceThreshold # mutual_info_regression\n",
    "fs = VarianceThreshold(0.001)\n",
    "df_nums = df_opt.select_dtypes(include=['float32','int8']) \n",
    "fs.fit(df_nums) # fit_transform(df)\n",
    "\n",
    "supported = fs.get_support()\n",
    "print('rows to remove:', len(supported[supported==False]))\n",
    "#print( df_opt['number_6'].value_counts() )\n",
    "for i in range(len(supported)):\n",
    "    if not supported[i]:\n",
    "        print(df_opt.columns[i])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "from sklearn.feature_selection import mutual_info_regression\n",
    "df_X =df_nums.drop('target',axis=1)\n",
    "df_y = df.target\n",
    "mir = mutual_info_regression(df_X[:1000], df_y[:1000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "plt.plot(mir)\n",
    "#print(len(mir[mir>0.02]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_X.columns[mir>np.mean(mir) + 4*np.std(mir)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "constant_columns = [\n",
    "        col_name\n",
    "         for col_name in df_opt.columns\n",
    "         if df_opt[col_name].nunique() == 1\n",
    "        ]\n",
    "constant_columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_opt.isnull().values.any())\n",
    "\n",
    "#df.fillna(-1, inplace=True)\n",
    "#print(df.isnull().values.any())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time\n",
    "import datetime\n",
    "\n",
    "\n",
    "empty_date = datetime.datetime.strptime('0001-01-01', '%Y-%m-%d')\n",
    "\n",
    "\n",
    "def parse_dt(x):\n",
    "    if not isinstance(x, str):\n",
    "        return empty_date\n",
    "    elif len(x) == len('2010-01-01'):\n",
    "        return datetime.datetime.strptime(x, '%Y-%m-%d')\n",
    "    elif len(x) == len('2010-01-01 10:10:10'):\n",
    "        return datetime.datetime.strptime(x, '%Y-%m-%d %H:%M:%S')\n",
    "    else:\n",
    "        return empty_date\n",
    "    \n",
    "\n",
    "def transform_datetime_features(df):\n",
    "    datetime_columns = [\n",
    "        col_name\n",
    "        for col_name in df.columns\n",
    "        if col_name.startswith('datetime')\n",
    "    ]\n",
    "\n",
    "    df_date = pd.DataFrame() #(dtypes=[['datetime','uint8','uint8','uint8','uint16']])\n",
    "    for col_name in datetime_columns:\n",
    "        #df_date[col_name],\n",
    "        #df_date['number_weekday_{}'.format(col_name)],\n",
    "        #df_date['number_month_{}'.format(col_name)],\n",
    "        #df_date['number_day_{}'.format(col_name)],\n",
    "        #df_date['number_hour_{}'.format(col_name)],\n",
    "        #df_date['number_hour_of_week_{}'.format(col_name)],\n",
    "        #df_date['number_minute_of_day_{}'.format(col_name)] = \n",
    "        df[col_name].apply(lambda x: parse_dt(x)[0] )\n",
    "        \n",
    "        #df_date[col_name] = a[0]\n",
    "        #df_date['number_weekday_{}'.format(col_name)] = df_date[col_name].apply(lambda x: x.weekday())\n",
    "        #df_date['number_month_{}'.format(col_name)] = df_date[col_name].apply(lambda x: x.month)\n",
    "        #df_date['number_day_{}'.format(col_name)] = df_date[col_name].apply(lambda x: x.day)\n",
    "        #f_date['number_hour_{}'.format(col_name)] = df_date[col_name].apply(lambda x: x.hour)\n",
    "        #df_date['number_hour_of_week_{}'.format(col_name)] = df_date[col_name].apply(lambda x: x.hour + x.weekday() * 24)\n",
    "        #df_date['number_minute_of_day_{}'.format(col_name)] = df_date[col_name].apply(lambda x: x.minute + x.hour * 60)        \n",
    "        \n",
    "    return transform_ds(df_date)\n",
    "\n",
    "\"\"\"\n",
    "apply(lambda x: parse_dt(x)) (FAST):\n",
    "<class 'pandas.core.frame.DataFrame'>\n",
    "RangeIndex: 143525 entries, 0 to 143524\n",
    "Columns: 427 entries, number_weekday_datetime_0 to datetime_90\n",
    "dtypes: datetime64[ns](90), float32(337)\n",
    "memory usage: 283.1 MB\n",
    "Wall time: 6min 16s\n",
    "\n",
    "pd.to_datetime (SLOW):\n",
    "<class 'pandas.core.frame.DataFrame'>\n",
    "RangeIndex: 143525 entries, 0 to 143524\n",
    "Columns: 630 entries, number_weekday_datetime_0 to datetime_90\n",
    "dtypes: datetime64[ns](90), int16(174), int8(366)\n",
    "memory usage: 196.3 MB\n",
    "Wall time: 8min 34s\n",
    "\"\"\"\n",
    "\n",
    "#df_dates = transform_datetime_features(df_opt)\n",
    "#df_dates.info(memory_usage='deep')\n",
    "str(datetime.MINYEAR).rjust(4,'0')\n",
    "str(20183).rjust(4,'0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_opt['datetime_3']\n",
    "#d = pd.DataFrame(dtype=[['datetime','uint8','uint8','uint8','uint8','uint8','uint16']])\n",
    "#type(df_opt['datetime_0'])\n",
    "#pd.Series((None,None,None,None,None,None,None))\n",
    "pd.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_opt[df_dates.columns] = df_dates\n",
    "df_opt.info(memory_usage='deep')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt_dates = transform_ds(pd.DataFrame())\n",
    "#df_dates.info(memory_usage='deep')\n",
    "#df_dates.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_opt.info(memory_usage='deep')\n",
    "df_opt.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "df_int = df.select_dtypes(include=['int','int64'])\n",
    "df_opt = df_int.apply(pd.to_numeric, downcast='unsigned')\n",
    "print(df_opt.shape)\n",
    "print(df_int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_float = df.select_dtypes(include=['float'])\n",
    "df_opt = pd.concat( [df_opt, df_float.apply(pd.to_numeric, downcast='float')], axis=1)\n",
    "print(df_opt.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in df.columns:\n",
    "    print(df.dtypes[col])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def estimate_rows( file_name ):\n",
    "    nrows = 200\n",
    "    test_file_name = 'test_row_count.csv'\n",
    "\n",
    "    file_size = os.path.getsize(file_name)\n",
    "    df = pd.read_csv(file_name, nrows=nrows)\n",
    "    df.to_csv(test_file_name, header=False)\n",
    "    row_size = os.path.getsize(test_file_name) / nrows\n",
    "    rows = file_size / row_size\n",
    "    size = rows * sys.getsizeof(df) / nrows\n",
    "    return { 'rows': int(rows), 'row_size': int(row_size), 'total_size': int(size) }\n",
    "\n",
    "#estimate_rows(r'..\\check_1_r\\train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "scaler.transform()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import GradientBoostingRegressor, GradientBoostingClassifier\n",
    "reg = GradientBoostingRegressor()\n",
    "cls = GradientBoostingClassifier()\n",
    "print( reg.get_params().keys() )\n",
    "print()\n",
    "print( cls.get_params().keys() )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "#?GridSearchCV\n",
    "?GradientBoostingRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import smtplib\n",
    "from email.mime.text import MIMEText\n",
    "\n",
    "msg = MIMEText('test')\n",
    "msg['Subject'] = 'subj'\n",
    "msg['From'] = 'pomka@yandex.ru'\n",
    "msg['To'] = 'pomka@yandex.ru'\n",
    "\n",
    "HOST = \"smtp.yandex.ru\"\n",
    "server = smtplib.SMTP(HOST)\n",
    "\n",
    "username = 'pomka'\n",
    "server.starttls()\n",
    "server.login(username, input())\n",
    "server.sendmail(msg['From'], msg['To'], msg.as_string())\n",
    "server.quit()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_cols = rnd_cols.copy()\n",
    "data_cols.append('zero')\n",
    "#from sklearn.decomposition import PCA\n",
    "#pca = PCA()\n",
    "#pca.fit_transform(df)\n",
    "#pca.explained_variance_ratio_\n",
    "from sklearn.feature_selection import VarianceThreshold # mutual_info_regression\n",
    "fs = VarianceThreshold(0.001)\n",
    "a2 = fs.fit_transform(df[data_cols])\n",
    "print(len(fs.variances_))\n",
    "print(a2.shape)\n",
    "#print( df.columns )\n",
    "#GOOD:\n",
    "#mir = mutual_info_regression(df[rnd_cols], df['multi']) !\n",
    "#print(mir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "rows = 10**4\n",
    "cols = 100\n",
    "a = np.random.randn(rows, cols)\n",
    "rnd_cols = list(map( str, range(1,cols+1) ))\n",
    "#print(rnd_cols)\n",
    "df = pd.DataFrame(a, columns=rnd_cols)\n",
    "\n",
    "#.corr()\n",
    "df['depend'] = (df['1']+df['2']) / (df['3'] * df['4'])\n",
    "df['zero'] = np.zeros(rows)\n",
    "#df.corr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "from sklearn.feature_selection import mutual_info_regression\n",
    "mir = mutual_info_regression(df[rnd_cols], df['depend'])\n",
    "print(mir)\n",
    "plt.plot(mir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df[rnd_cols].columns[mir>np.mean(mir)]) # + 1*np.std(mir)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(mir[mir > 0.01])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "df_unique = df_opt.apply(lambda x: x.nunique())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_unique.sort_values('counts', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_unique"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_unique = df_unique[df_unique > 2]\n",
    "df_unique = df_unique[df_unique <= 20]\n",
    "df_unique.sort_values(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_unique['number_597']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "X=[0.6,0.4,0.58,0.1,-1130]\n",
    "m=['a','b','c','d','e']\n",
    "#sorted_scores = sorted(scores)[::-1]\n",
    "#[x for x in X if x>np.mean(X)]\n",
    "m[3:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(r'..\\check_1_r\\train.csv', nrows=None, low_memory=False)\n",
    "constant_columns = [\n",
    "        col_name\n",
    "        for col_name in df.columns\n",
    "        if df[col_name].nunique() == 1\n",
    "    ]\n",
    "constant_columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from zipfile import ZipFile\n",
    "folder = '003-no-ext-libs\\\\'\n",
    "#%cd $folder\n",
    "zip_file_name = folder + time.strftime(\"%Y-%m-%d_%H-%M-%S\", time.localtime()) + '_submission.zip'\n",
    "    \n",
    "files = [\n",
    "    'metadata.json',\n",
    "    'predict.py',\n",
    "    'train.py',\n",
    "    'utils.py']\n",
    "\n",
    "with ZipFile(zip_file_name, mode='w') as submission:\n",
    "    for file in files:\n",
    "        submission.write(folder + file,arcname=file)\n",
    "submission.close()    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df =  pd.read_csv(r'..\\check_3_r\\train.csv')\n",
    "if any(df.isnull()):\n",
    "    df.fillna(-1, inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(df.columns))\n",
    "for n in range(100,1000,100):\n",
    "    df_X = df[:n]\n",
    "    df_unique = df_X.apply(lambda x: x.nunique())\n",
    "    df_const = df_unique[df_unique == 1]\n",
    "\n",
    "    print(n, len(df_X.columns) - len(df_const))\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import traceback\n",
    "try:\n",
    "    raise Exception('ALL FEATURES DROPPED, STOPPING')\n",
    "except BaseException as e:\n",
    "    print(e)\n",
    "    print(traceback.format_exc())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "est = GradientBoostingClassifier()\n",
    "type(est)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import hashlib\n",
    "columns = b'dwefwefwefwe'\n",
    "hash_digest = hashlib.md5(columns).hexdigest()\n",
    "print(hash_digest.upper())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "!SET OMP_NUM_THREADS=4\n",
    "size = 10000\n",
    "a = np.random.random_sample((size, size))\n",
    "b = np.random.random_sample((size, size))\n",
    "n = np.dot(a,b)\n",
    "#np.show_config()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
